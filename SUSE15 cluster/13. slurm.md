## 目錄
* [目錄](#目錄)
* [hosts](#hosts)
* [Install](#Install)
* [slurm_conf](#slurm_conf)
* [QOS](#OOS)
* [注意事項](#注意事項)
---

## hosts

hosts 不一定要安裝slurm時才設置，可以在系統架設好後就設定(ex: ssh 時可以不用打ip)

```bash
# server
server :~ # vi /etc/hosts
...
127.0.0.1       localhost
...
(外網 ip)       jjc
192.168.122.25  jjc
192.168.122.28  client1 client1
192.168.122.29  client2 client2
192.168.122.30  client3 client3
192.168.122.31  client4 client4
192.168.122.24  testk1

# client
client :~ # vi /etc/hosts
...
127.0.0.1       localhost
...
192.168.122.25  jjc jjc
192.168.122.28  client1 client1
192.168.122.29  client2 client2
192.168.122.30  client3 
192.168.122.31  client4 client4
192.168.122.24  testk1

● 要確保 server 與 client 的 hosts 都設定完成，否則後續 slurm 的服務會有問題
```
---

## Install

```bash
# server (controller)
server :~ # zypper in slurm
server :~ # systemctl enable slurmctld
server :~ # systemctl start slurmctld

# client (node)
client :~ # zypper in slurm-node
client :~ # systemctl enable slurmd
client :~ # systemctl start slurmd

● client 上 slurmd.service 等 slurm.conf 設定完成後再進行啟用
```

---

## slurm_conf

```bash
server :~ # vi /etc/slurm/slurm.conf

# ControlMachine
#ControlMachine=jjc                               # serverhostname

# BackupServer                                    # 利用 backup 功能創建2個server
SlurmctldHost=jjc(192.168.122.25)                 # 管理者 root 登入用之server(k0)
SlurmctldHost=testk1(192.168.122.24)              # 使用者 user 登入用之server(k1)

# port config
SlurmctldPort=6817
SlurmdPort=6818

# logfile config
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdLogFile=/var/log/slurmd.log

# Priority config (QOS)
PriorityWeightQOS=1000                            # 配合 QOS 計算 Priority 之參數 (參考下方注意事項)

# privatedata config
privatedata=jobs                                  # 使用者只能查詢到自己的 jobs 狀態

# node config
NodeName=client1 State=idle CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1
NodeName=client2 State=idle CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1
NodeName=client3 State=idle CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1
NodeName=client4 State=idle CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1

# partition config
PartitionName=test Nodes=client1 MaxTime=24:00:00 State=UP
PartitionName=defult Nodes=client2 MaxTime=24:00:00 State=UP Default=YES
PartitionName=gpu Nodes=client[3-4] MaxTime=24:00:00 State=UP AllowGroups=jjcgpu,jjcadmin QOS=jjcqos5

# Defult = YES 則為預設之 Partition ，若沒有設定會由最下方的 Partition 作為預設值
# AllowGroups 允許那些 groups 的使用者可以使用該 Partition 這邊的 groups 就是 /etc/group 中設定的群組

● 這邊將兩台 server 分別提供給 root 及 user 登入分流 ， 方便管理者進行管理
● 限制使用者僅能觀看自己的 jobs
● 模擬不同的機器組成的 Partition 限制使用者、預設值等等
● QOS的設定會在後方進行說明

```

```bash
server :~ # scp /etc/slurm/slurm.conf root@client3:/etc/slurm/.

client :~ # systemctl enable slurmd
client :~ # systemctl start slurmd

# test
server :~ # srun -N4 hostname
client1
client2
client3
client4

● 把 slurm.conf 複製到每一台client上
● md5sum 確認是否相同

```

---

## QOS

我們可以藉由修改slurm.conf以及sbatch的script中的參數來達到初步的計算資源分配  
QOS可以進一步的幫助我們分配計算資源，例如設定每個使用者同時可submit的作業總數、優先級(priority)等等  

● 查詢Cluster
```bash
server :~ # sacctmgr list cluster                        # 查询Cluster
   Cluster     ControlHost  ControlPort   RPC     Share GrpJobs       GrpTRES GrpSubmit MaxJobs       MaxTRES MaxSubmit     MaxWall                  QOS   Def QOS
---------- --------------- ------------ ----- --------- ------- ------------- --------- ------- ------------- --------- ----------- -------------------- ---------
       cl1                            0     0         1                                                                                           normal
   jjcfarm                            0     0         1                                                                                           normal


server :~ # sacctmgr list cluster format=Cluster,QOS     # 查询Cluster (依照特定格式format=XXX)
   Cluster                  QOS
---------- --------------------
       cl1               normal
   jjcfarm               normal
```

● 創建新Cluster
```bash
server :~ # sacctmgr add cluster linux                   # 創建名為 linux 的 Cluster
 Adding Cluster(s)
  Name           = linux
Would you like to commit changes? (You have 30 seconds to decide)
(N/y): y

server :~ # sacctmgr list cluster format=Cluster,QOS
   Cluster                  QOS
---------- --------------------
       cl1               normal
   jjcfarm               normal
     linux               normal
```
● 查詢/添加Account/User

```bash
server :~ # sacctmgr list account                              # 查詢Account
server :~ # sacctmgr list account format=Cluster,Account,User  # 查詢Account(特定格式)

sacctmgr add user name=test1 account=jjcusers cluster=linux    # 新增一個名為 jjcusers 的 account 到 linux 中
 Adding Account(s)
  jjcusers
 Settings
  Description     = Account Name
  Organization    = Parent/Account Name
 Associations
  A = jjcusers   C = linux
Would you like to commit changes? (You have 30 seconds to decide)
(N/y): y

server :~ # sacctmgr add user name=test1 account=jjcusers cluster=linux  # 將test1(使用者) 新增到 jjcusers 及 linux中
 Associations =
  U = test1     A = jjcusers   C = linux
 Non Default Settings
Would you like to commit changes? (You have 30 seconds to decide)
(N/y): y

server :~ # sacctmgr list account format=Cluster,Account
   Cluster    Account
---------- ----------
                 acc1
             jjcusers
                 root
                 
server :~ # sacctmgr list assoc format=Cluster,Account,User,QOS
   Cluster    Account       User        QOS
---------- ---------- ---------- ----------
       cl1       root                normal
       cl1       root       root     normal
       cl1       acc1                  qos1
       cl1       acc1      test1       qos1
   jjcfarm       root                normal
   jjcfarm       root       root     normal
     linux       root                normal
     linux       root       root     normal
     linux   jjcusers                normal
     linux   jjcusers      test1     normal

● 可以看到最後一行有我們剛剛新增的 user

```

● QOS設定

```bash
server :~ # sacctmgr add qos jjcqos1                         # 新增一個名為jjcqos1的 QOS
 Adding QOS(s)
  jjcqos1
 Settings
  Description    = jjcqos1
Would you like to commit changes? (You have 30 seconds to decide)
(N/y): y

# 將QOS jjcqos1 的優先級調整為10 (數字越小優先級越高)
server :~ # sacctmgr modify qos jjcqos1 set priority=10

# 將QOS jjcqos1 Job數量上限設定為2 (若submit超過2個則會等待)
server :~ # sacctmgr modify qos jjcqos1 set GrpJobs=2

server :~ # sacctmgr show qos format=name,priority,GrpJobs
      Name   Priority GrpJobs
---------- ---------- -------
    normal          0
      qos1       1000
      qos2          0
   jjcqos1         10       2

server :~ # sacctmgr modify user test1 set qos=jjcqos1      # 將 test1 這個user的 QOS 設置為 jjcqos1
server :~ # sacctmgr show assoc format=cluster,user,qos
   Cluster       User                  QOS
---------- ---------- --------------------
     linux                          normal
     linux       root               normal
     linux                          normal
     linux      test1              jjcqos1


● 可以看到最後一行 test1 的 QOS 已經被變更
● 這時候如果使用test1的身分submit4個job，會發現就算計算資源是足夠的，還是只有2個job正在計算，其餘job則會waiting

```


---



## 注意事項
● QOS Test

最初設定QOS之後有發現雖然在 sacctmgr show qos 及 sacctmgr show assoc 都能看到使用者已經被設定成正確的qos  
test1設定為jjcqos1， test2 設定為jjcqos2  
但是執行排程的時候 GrpJobs 及 Priority 沒有確實執行  

以下簡略說明解決的方法：  

● GrpJobs Test：  
我先將兩個使用者的priority都設定相同結果，再分別 submit 4個job (皆為1node/2core)觀察結果  
在這邊test1的GrpJobs為2，test2的GrpJobs為1  
導致先前設定完成卻沒有執行GrpJobs的原因可能如下  
在slurm.conf中需要加入AccountingStorageEnforce的設置，一併重啟slumctld及slurmdbd的服務  
```bash
server :~ # cat /erc/slurm/slurn.conf

AccountingStorageEnforce=limits

 ```
 AccountingStorageEnforce=limits 會強制設置 associations 和 qos  
 關於AccountingStorageEnforce的詳細參數>>>[連結](https://slurm.schedmd.com/accounting.html)  
 
 測試結果：

![image](https://github.com/HongScarlet/homework/blob/master/SUSE15%20cluster/img/Slurm/note-1.png)  

![image](https://github.com/HongScarlet/homework/blob/master/SUSE15%20cluster/img/Slurm/note-2.png)  

可以看到2個使用者同時計算的Job數目被成功限制

● Proirity Test： 
處理完 GrpJobs 後我將使用者改為不同的 Proirity 進行比較  
利用scontrol show job 查詢 root,test1,test2 送出的job發現三者Proirity都為1(沒有區別)  
根據 slurm 的 doc>>>[連結](https://slurm.schedmd.com/priority_multifactor.html#general)  

```bash
Job_priority =
	site_factor +
	(PriorityWeightAge) * (age_factor) +
	(PriorityWeightAssoc) * (assoc_factor) +
	(PriorityWeightFairshare) * (fair-share_factor) +
	(PriorityWeightJobSize) * (job_size_factor) +
	(PriorityWeightPartition) * (partition_factor) +
	(PriorityWeightQOS) * (QOS_factor) +
	SUM(TRES_weight_cpu * TRES_factor_cpu,
	    TRES_weight_<type> * TRES_factor_<type>,
	    ...)
	- nice_factor
```
由於我們 slurm.conf 中沒有任何與 PriorityWeight 相關的設置  
我試著加上 PriorityWeightQOS ，並重啟 slurmctld 再進行觀察  

```bash
server :~ # cat /erc/slurm/slurn.conf

PriorityWeightQOS=1000

 ```
 
我將 test1 的優先級設置的比 test2 高  
測試的方法是先用root,test1,test2 各submit 1個 job (皆為4node/2core)  

![image](https://github.com/HongScarlet/homework/blob/master/SUSE15%20cluster/img/Slurm/note-3.png)  

第一次測試 submit 順序是 root > test1 > test2  
第二次測試 submit 順序是 root > test2 > test1  

再將root的 job cancel，如果Priority有發揮作用 兩次實驗應當都是 test1 會優先執行  

第一次測試：
root (1525) > test1 (1526) > test2 (1527)  
![image](https://github.com/HongScarlet/homework/blob/master/SUSE15%20cluster/img/Slurm/note-4.png)  

這時scontrol show job 查詢可以看到三者的 proirity 已經不同了 test1(1000) > test2 (500)

![image](https://github.com/HongScarlet/homework/blob/master/SUSE15%20cluster/img/Slurm/note-5.png)  

![image](https://github.com/HongScarlet/homework/blob/master/SUSE15%20cluster/img/Slurm/note-6.png)  

第二次測試：
root (1528) > test2 (1529) > test1 (1530)  
可以看到 test1 的 job 雖然比 test2 晚送出(id 較大)，因為proirity的緣故，所以較早被計算
![image](https://github.com/HongScarlet/homework/blob/master/SUSE15%20cluster/img/Slurm/note-7.png) 




