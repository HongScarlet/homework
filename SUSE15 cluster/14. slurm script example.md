## 目錄
* [目錄](#目錄)
* [簡述](#簡述)
* [vsub_gpu_544](#vsub-gpu-544)
* [vsub_test_544](#vsub-test-544)
* [vsub_defult_544](#vsub-defult-544)
* [vasp script old](#vasp-script-old)
* [g09_script_old](#g09-script-old)
* [注意事項](#注意事項)
---
## 簡述
● 我們在先前 slurm 的設定中把 node 分成3個不同的part，這邊以 vasp 為例子寫了3個不同的 script(送往3個part)  
● 執行 script 會先詢問每個node想要使用幾個core  
● 執行 script 後會依照輸入的參數生成 XXX.slurm 檔案 並且透過 sbatch XXX.slurm 將 job 送出  
● vsub_gpu_544 與 vsub_test_544 有指定 partition  
● vsub_deflut_544 由於是送到 deflut partition 因此 script 中沒有設定  
● 使用者執行時  server :~ # vasp_gpu_544 {jobname} >>>生成 jobname.slurm >> sbatch jobname.slurm  

## vsub gpu 544

```bash
server :~ # cat vsub_gpu_544

#!/bin/bash

JOB=$1

echo "How many ntasks-per-node do you want to run your job?(1,2) [1]"
read ntasks_per_node
ntasks_per_node=${ntasks_per_node:-1}

cat <<EOF > $JOB.slurm
#!/bin/bash
#SBATCH -J $JOB                ## Job Name
#SBATCH -o $JOB.out            ## standard output
#SBATCH -e $JOB.err            ## standard error
#SBATCH -t 24:00:00            ## time for your job: 1 day
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=$ntasks_per_node
#SBATCH --partition=gpu

source /work/intel/compilers_and_libraries/linux/bin/compilervars.sh intel64
source /work/intel/compilers_and_libraries/linux/mkl/bin/mklvars.sh intel64
source /work/intel/compilers_and_libraries/linux/mpi/intel64/bin/mpivars.sh intel64

ulimit -s unlimited

mpiexec.hydra -hosts-group \$SLURM_JOB_NODELIST -n \$SLURM_NTASKS -ppn \$SLURM_NTASKS_PER_NODE ./vasp_std

EOF

chmod u+x $JOB.slurm
sbatch $JOB.slurm

```

## vsub test 544

```bash
server :~ # cat vsub_gpu_544

#!/bin/bash

JOB=$1

echo "How many ntasks-per-node do you want to run your job?(1,2) [1]"
read ntasks_per_node
ntasks_per_node=${ntasks_per_node:-1}

cat <<EOF > $JOB.slurm
#!/bin/bash
#SBATCH -J $JOB                ## Job Name
#SBATCH -o $JOB.out            ## standard output
#SBATCH -e $JOB.err            ## standard error
#SBATCH -t 24:00:00            ## time for your job: 1 day
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=$ntasks_per_node
#SBATCH --partition=test

source /work/intel/compilers_and_libraries/linux/bin/compilervars.sh intel64
source /work/intel/compilers_and_libraries/linux/mkl/bin/mklvars.sh intel64
source /work/intel/compilers_and_libraries/linux/mpi/intel64/bin/mpivars.sh intel64

ulimit -s unlimited

mpiexec.hydra -hosts-group \$SLURM_JOB_NODELIST -n \$SLURM_NTASKS -ppn \$SLURM_NTASKS_PER_NODE ./vasp_std

EOF

chmod u+x $JOB.slurm
sbatch $JOB.slurm

```

## vsub defult 544

```bash
server :~ # cat vsub_gpu_544

#!/bin/bash

JOB=$1

echo "How many ntasks-per-node do you want to run your job?(1,2) [1]"
read ntasks_per_node
ntasks_per_node=${ntasks_per_node:-1}

cat <<EOF > $JOB.slurm
#!/bin/bash
#SBATCH -J $JOB                ## Job Name
#SBATCH -o $JOB.out            ## standard output
#SBATCH -e $JOB.err            ## standard error
#SBATCH -t 24:00:00            ## time for your job: 1 day
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=$ntasks_per_node

source /work/intel/compilers_and_libraries/linux/bin/compilervars.sh intel64
source /work/intel/compilers_and_libraries/linux/mkl/bin/mklvars.sh intel64
source /work/intel/compilers_and_libraries/linux/mpi/intel64/bin/mpivars.sh intel64

ulimit -s unlimited

mpiexec.hydra -hosts-group \$SLURM_JOB_NODELIST -n \$SLURM_NTASKS -ppn \$SLURM_NTASKS_PER_NODE ./vasp_std

EOF

chmod u+x $JOB.slurm
sbatch $JOB.slurm

```

## vasp script old

```bash
server :~ # cat vasp.slurm

#!/bin/bash
#SBATCH -J vasp_test           ## Job Name
#SBATCH -o %j.out              ## standard output
#SBATCH -e %j.err              ## standard error
#SBATCH -t 24:00:00            ## time for your job: 1 day
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=2

source /work/intel/compilers_and_libraries/linux/bin/compilervars.sh intel64
source /work/intel/compilers_and_libraries/linux/mkl/bin/mklvars.sh intel64
source /work/intel/compilers_and_libraries/linux/mpi/intel64/bin/mpivars.sh intel64

ulimit -s unlimited

mpiexec.hydra -hosts-group $SLURM_JOB_NODELIST -ppn $SLURM_NTASKS_PER_NODE hostname>host.XXXXX
mpiexec.hydra -hosts-group $SLURM_JOB_NODELIST -n $SLURM_NTASKS -ppn $SLURM_NTASKS_PER_NODE ./vasp_std

```
---

## g09 script old

```bash
server :~ # cat g09.slurm

#!/bin/bash
#SBATCH --job-name=gaussianjob
#SBATCH --output=gaussian.out
#SBATCH --error=gaussian.err
#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1

source /work/intel/compilers_and_libraries/linux/bin/compilervars.sh intel64
source /work/intel/compilers_and_libraries/linux/mkl/bin/mklvars.sh intel64
source /work/intel/compilers_and_libraries/linux/mpi/intel64/bin/mpivars.sh intel64

source /work/g09/bsd/g09.profile

ulimit -s unlimited

g09 C2H4-opt-freq.com

```

---

## 注意事項
